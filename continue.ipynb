{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code in this notebook to finetune the model at a certain epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.44.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.44.2)\n",
      "Collecting huggingface\n",
      "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (0.6.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.44.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.44.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.44.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.44.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.44.2) (2025.10.5)\n",
      "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: huggingface\n",
      "Successfully installed huggingface-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.44.2 huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "EPOCHS = 2 # SET IT ACCORDINGLY\n",
    "learning_rate = 1e-5\n",
    "image_size = 128\n",
    "latent_size = image_size // 16\n",
    "data_path = \"/teamspace/studios/this_studio/coco\"\n",
    "# START FROM EPOCH 9 WITH LR 1e-5\n",
    "checkpoint_to_resume = \"./ldm_checkpoints/epoch_9\"  # S pecify your checkpoint\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Tokenizer and Transforms\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\", chat_template=None)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "max_length = 77\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class (same as your training script)\n",
    "class CocoWithAnnotations(Dataset):\n",
    "    def __init__(self, path, tokenizer, transform, train=True):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.data = None\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train = train\n",
    "        if self.data is None:\n",
    "            self.open_json()\n",
    "\n",
    "    def open_json(self):\n",
    "        split = \"train\" if self.train else \"val\"\n",
    "        with open(f'{self.path}/annotations/captions_{split}2014.json', 'r') as stream:\n",
    "            self.data = json.load(stream)['annotations']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        annot = self.data[index]\n",
    "        img_id = str(annot['image_id']).zfill(12)\n",
    "        split = \"train\" if self.train else \"val\"\n",
    "        image_path = f'{self.path}/{split}2014/COCO_{split}2014_{img_id}.jpg'\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        text_emb = self.tokenizer(\n",
    "            annot['caption'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return image, text_emb.input_ids.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# DataLoader\n",
    "def collate_fn(batch):\n",
    "    batch = [(img, cap) for img, cap in batch if img is not None and cap is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    images, captions = zip(*batch)\n",
    "    return torch.stack(images), torch.stack(captions)\n",
    "\n",
    "dataset = CocoWithAnnotations(data_path, tokenizer, transform, train=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learninig rate = 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "/tmp/ipykernel_13577/1528010809.py:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea95a3e5cbc54256ac185ceffb2bf9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/11:   0%|          | 0/51765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13577/1528010809.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/tmp/ipykernel_13577/1528010809.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed with average loss: 0.169911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5402f19f0e3048ccacbfd37480e8ae3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/11:   0%|          | 0/51765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 completed with average loss: 0.168977\n",
      "Training resumed and completed!\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "print(f'learninig rate = {learning_rate}')\n",
    "unet = UNet2DConditionModel.from_pretrained(os.path.join(checkpoint_to_resume, \"unet\")).to(device)\n",
    "autoencoder = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "diffusion = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")\n",
    "\n",
    "# Freeze VAE and text encoder\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(os.path.join(checkpoint_to_resume, \"training_state.pth\"))\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "START_EPOCH = checkpoint['epoch'] + 1  # Start from the next epoch\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(START_EPOCH, START_EPOCH + EPOCHS):\n",
    "    unet.train()\n",
    "    epoch_loss = 0\n",
    "    valid_batches = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{START_EPOCH + EPOCHS}\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        if batch is None:\n",
    "            print(f\"[WARN] Batch {step}: NoneType batch skipped.\")\n",
    "            continue\n",
    "\n",
    "        images, captions = batch\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        captions = captions.to(device, non_blocking=True)\n",
    "\n",
    "        # Sanity check on image inputs\n",
    "        if not torch.isfinite(images).all():\n",
    "            print(f\"[ERROR] Non-finite values in images at step {step}, skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    latents = autoencoder.encode(images).latent_dist.sample()\n",
    "                    latents = latents * 0.18215\n",
    "\n",
    "                if not torch.isfinite(latents).all():\n",
    "                    print(f\"[ERROR] NaN/Inf in latents at step {step}, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0, diffusion.config.num_train_timesteps, \n",
    "                    (latents.shape[0],), device=device\n",
    "                )\n",
    "\n",
    "                noisy_latents = diffusion.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                if not torch.isfinite(noisy_latents).all():\n",
    "                    print(f\"[ERROR] NaN/Inf in noisy_latents at step {step}, skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(captions)[0]\n",
    "\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[EXCEPTION] Error at step {step}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Force loss to run in full precision (helps avoid AMP overflow)\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            loss = torch.nn.functional.mse_loss(\n",
    "                noise_pred.float(), noise.float(), reduction=\"mean\"\n",
    "            )\n",
    "\n",
    "        # Detect NaN loss early\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"[WARN] Non-finite loss detected (step {step}), skipping optimizer step.\")\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(unet.parameters(), 0.5)\n",
    "\n",
    "        try:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        except Exception as e:\n",
    "            print(f\"[EXCEPTION] Optimizer/Scaler step failed at step {step}: {e}\")\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.6f}\"})\n",
    "\n",
    "        # Optional: print detailed debug info occasionally\n",
    "        # if step % 500 == 0:\n",
    "        #     print(f\"[DEBUG] Step {step}: loss={loss.item():.6f}, scale={scaler.get_scale()}\")\n",
    "\n",
    "    avg_loss = epoch_loss / max(valid_batches, 1)\n",
    "    print(f\"Epoch {epoch + 1} completed with average loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "    checkpoint_dir = f\"./ldm_checkpoints/epoch_{epoch + 1}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    unet.save_pretrained(os.path.join(checkpoint_dir, \"unet\"))\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': unet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss, # for logging\n",
    "    }, os.path.join(checkpoint_dir, \"training_state.pth\"))\n",
    "\n",
    "print(\"Training resumed and completed!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
