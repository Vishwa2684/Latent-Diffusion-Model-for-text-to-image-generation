{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers.models.attention_processor import AttnProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "MODEL_PATH = \"/teamspace/studios/this_studio/Latent-Diffusion-Model-for-text-to-image-generation/ldm_checkpoints/epoch_11\"\n",
    "OUTPUT_DIR = \"./openvino_models\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UNet...\n",
      "Loading Text Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VAE...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD MODELS\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading UNet...\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    os.path.join(MODEL_PATH, \"unet\"),\n",
    "    use_safetensors=True\n",
    ")\n",
    "unet.set_attn_processor(AttnProcessor())     # turn OFF optimized attention\n",
    "unet.to(torch.float32)               # ensure fp32\n",
    "unet.eval()\n",
    "\n",
    "print(\"Loading Text Encoder...\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE).eval()\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "print(\"Loading VAE...\")\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "\n",
    "    def forward(self, latent):\n",
    "        # This replicates SD's actual forward output\n",
    "        return self.vae.decode(latent).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPORT TO ONNX\n",
    "# ============================================================\n",
    "\n",
    "def export_unet():\n",
    "    print(\"\\n=== Exporting UNet to ONNX ===\")\n",
    "\n",
    "    # dummy_latent = torch.randn(2, 4, 16, 16)   # batch=2 for CFG (uncond + cond)\n",
    "    # dummy_timestep = torch.tensor([10], dtype=torch.int64)  # scheduler timestep dtype long\n",
    "    # dummy_context = torch.randn(2, 77, 512)   # CLIP text encoder output size = 512\n",
    "    dummy_latent = torch.randn(2, 4, 16, 16).float()\n",
    "    dummy_timestep = torch.tensor([10.0], dtype=torch.float32) \n",
    "    dummy_context = torch.randn(2, 77, 512).float()\n",
    "\n",
    "    onnx_path = os.path.join(OUTPUT_DIR, \"unet.onnx\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        unet,\n",
    "        (dummy_latent, dummy_timestep, dummy_context),\n",
    "        onnx_path,\n",
    "        input_names=[\"latent\", \"timestep\", \"context\"],\n",
    "        output_names=[\"noise_pred\"],\n",
    "        opset_version=17,\n",
    "        dynamic_axes={\n",
    "            \"latent\": {0: \"batch\", 2: \"h\", 3: \"w\"},\n",
    "            \"context\": {0: \"batch\"},\n",
    "        }\n",
    "    )\n",
    "    print(\"UNet exported:\", onnx_path)\n",
    "    return onnx_path\n",
    "\n",
    "# def export_text_encoder():\n",
    "#     print(\"\\n=== Exporting Text Encoder to ONNX ===\")\n",
    "#     # text_encoder.to(DEVICE)\n",
    "#     tokens = tokenizer(\n",
    "#         [\"a dummy sample text\"],\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=77,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     onnx_path = os.path.join(OUTPUT_DIR, \"text_encoder.onnx\")\n",
    "\n",
    "#     torch.onnx.export(\n",
    "#         text_encoder,\n",
    "#         (tokens[\"input_ids\"], tokens[\"attention_mask\"]),\n",
    "#         onnx_path,\n",
    "#         input_names=[\"input_ids\", \"attention_mask\"],\n",
    "#         output_names=[\"last_hidden_state\"],\n",
    "#         opset_version=17,\n",
    "#         dynamic_axes={\n",
    "#             \"input_ids\": {0: \"batch\"},\n",
    "#             \"attention_mask\": {0: \"batch\"}\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     print(\"Text Encoder exported:\", onnx_path)\n",
    "#     return onnx_path\n",
    "def export_text_encoder():\n",
    "    print(\"\\n=== Exporting Text Encoder to ONNX ===\")\n",
    "\n",
    "    # Move model to CPU for export\n",
    "    text_encoder_cpu = text_encoder.to(DEVICE).eval()\n",
    "\n",
    "    # Create CPU dummy inputs\n",
    "    tokens = tokenizer(\n",
    "        [\"a dummy sample text\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = tokens[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    onnx_path = os.path.join(OUTPUT_DIR, \"text_encoder.onnx\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        text_encoder_cpu,\n",
    "        (input_ids, attention_mask),\n",
    "        onnx_path,\n",
    "        input_names=[\"input_ids\", \"attention_mask\"],\n",
    "        output_names=[\"last_hidden_state\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\":       {0: \"batch\", 1: \"sequence\"},\n",
    "            \"attention_mask\":  {0: \"batch\", 1: \"sequence\"},\n",
    "            \"last_hidden_state\": {0: \"batch\", 1: \"sequence\"},\n",
    "        },\n",
    "        opset_version=17\n",
    "    )\n",
    "\n",
    "    print(\"Text Encoder exported:\", onnx_path)\n",
    "    return onnx_path\n",
    "\n",
    "# def export_vae_decoder():\n",
    "#     print(\"\\n=== Exporting VAE Decoder to ONNX ===\")\n",
    "\n",
    "#     dummy_latent = torch.randn(1, 4, 16, 16).to(DEVICE)   # correct latent resolution for 64Ã—64 VAE\n",
    "\n",
    "#     onnx_path = os.path.join(OUTPUT_DIR, \"vae_decoder.onnx\")\n",
    "\n",
    "#     torch.onnx.export(\n",
    "#         vae.decoder,\n",
    "#         (dummy_latent,),\n",
    "#         onnx_path,\n",
    "#         input_names=[\"latent\"],\n",
    "#         output_names=[\"image\"],\n",
    "#         opset_version=17,\n",
    "#         dynamic_axes={\n",
    "#             \"latent\": {0: \"batch\", 2: \"h\", 3: \"w\"},\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     print(\"VAE Decoder exported:\", onnx_path)\n",
    "#     return onnx_path\n",
    "\n",
    "def export_vae_decoder():\n",
    "    print(\"\\n=== Exporting VAE Decoder to ONNX ===\")\n",
    "\n",
    "    vae_cpu = VAEDecoderWrapper(vae).to(DEVICE).eval()\n",
    "\n",
    "    dummy_latent = torch.randn(1, 4, 16, 16, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    onnx_path = os.path.join(OUTPUT_DIR, \"vae_decoder.onnx\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        vae_cpu,\n",
    "        (dummy_latent,),\n",
    "        onnx_path,\n",
    "        input_names=[\"latent\"],\n",
    "        output_names=[\"image\"],\n",
    "        dynamic_axes={\n",
    "            \"latent\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
    "            \"image\":  {0: \"batch\"},\n",
    "        },\n",
    "        opset_version=17\n",
    "    )\n",
    "\n",
    "    print(\"VAE Decoder exported:\", onnx_path)\n",
    "    return onnx_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# CONVERT ONNX â†’ OPENVINO IR\n",
    "# ============================================================\n",
    "\n",
    "def convert_to_openvino(onnx_path):\n",
    "    print(f\"\\n=== Converting {onnx_path} to OpenVINO IR ===\")\n",
    "\n",
    "    command = [\n",
    "        \"mo\",\n",
    "        \"--input_model\", onnx_path,\n",
    "        \"--output_dir\", OUTPUT_DIR\n",
    "    ]\n",
    "\n",
    "    subprocess.run(command, check=True)\n",
    "    print(\"Converted:\", onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Exporting UNet to ONNX ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4559/2461332435.py:17: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet exported: ./openvino_models/unet.onnx\n",
      "\n",
      "=== Exporting Text Encoder to ONNX ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4559/2461332435.py:79: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Encoder exported: ./openvino_models/text_encoder.onnx\n",
      "\n",
      "=== Exporting VAE Decoder to ONNX ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4559/2461332435.py:127: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Decoder exported: ./openvino_models/vae_decoder.onnx\n",
      "\n",
      "=== Converting ./openvino_models/unet.onnx to OpenVINO IR ===\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "Check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2023_bu_IOTG_OpenVINO-2023-1&content=upg_all&medium=organic or on https://github.com/openvinotoolkit/openvino\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /teamspace/studios/this_studio/Latent-Diffusion-Model-for-text-to-image-generation/openvino_models/unet.xml\n",
      "[ SUCCESS ] BIN file: /teamspace/studios/this_studio/Latent-Diffusion-Model-for-text-to-image-generation/openvino_models/unet.bin\n",
      "Converted: ./openvino_models/unet.onnx\n",
      "\n",
      "=== Converting ./openvino_models/text_encoder.onnx to OpenVINO IR ===\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "Check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2023_bu_IOTG_OpenVINO-2023-1&content=upg_all&medium=organic or on https://github.com/openvinotoolkit/openvino\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /teamspace/studios/this_studio/Latent-Diffusion-Model-for-text-to-image-generation/openvino_models/text_encoder.xml\n",
      "[ SUCCESS ] BIN file: /teamspace/studios/this_studio/Latent-Diffusion-Model-for-text-to-image-generation/openvino_models/text_encoder.bin\n",
      "Converted: ./openvino_models/text_encoder.onnx\n",
      "\n",
      "=== Converting ./openvino_models/vae_decoder.onnx to OpenVINO IR ===\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "Check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2023_bu_IOTG_OpenVINO-2023-1&content=upg_all&medium=organic or on https://github.com/openvinotoolkit/openvino\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /teamspace/studios/this_studio/Latent-Diffusion-Model-for-text-to-image-generation/openvino_models/vae_decoder.xml\n",
      "[ SUCCESS ] BIN file: /teamspace/studios/this_studio/Latent-Diffusion-Model-for-text-to-image-generation/openvino_models/vae_decoder.bin\n",
      "Converted: ./openvino_models/vae_decoder.onnx\n",
      "\n",
      "ðŸŽ‰ All models successfully converted to OpenVINO!\n",
      "Files saved in: ./openvino_models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================\n",
    "unet_onnx = export_unet()\n",
    "text_onnx = export_text_encoder()\n",
    "vae_onnx = export_vae_decoder()\n",
    "\n",
    "convert_to_openvino(unet_onnx)\n",
    "convert_to_openvino(text_onnx)\n",
    "convert_to_openvino(vae_onnx)\n",
    "\n",
    "print(\"\\nðŸŽ‰ All models successfully converted to OpenVINO!\")\n",
    "print(f\"Files saved in: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
